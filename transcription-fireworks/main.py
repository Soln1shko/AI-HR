
import io
import json
import base64
import asyncio
import tempfile
import os
import traceback
import time
import requests 
import soundfile as sf 

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from starlette.websockets import WebSocketState

import numpy as np
import librosa
import logging

app = FastAPI()



def extract_features(audio: np.ndarray, sr: int):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã—Ö."""
    features = {}
    logging.info("–ù–∞—á–∞–ª–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    try:
        energy = librosa.feature.rms(y=audio)[0]
        features['avg_energy'] = np.mean(energy)
        features['energy_std'] = np.std(energy)
        features['energy_stability'] = 1 / (np.std(energy) + 0.001)
        pitches, magnitudes = librosa.piptrack(y=audio, sr=sr)
        pitch_values = pitches[pitches > 0]
        if len(pitch_values) > 0:
            features['avg_pitch'] = np.mean(pitch_values)
            features['pitch_std'] = np.std(pitch_values)
            features['pitch_stability'] = 1 / (np.std(pitch_values) + 0.1)
        else:
            features['avg_pitch'] = 0
            features['pitch_std'] = 0
            features['pitch_stability'] = 0
        tempo, _ = librosa.beat.beat_track(y=audio, sr=sr)
        features['tempo'] = tempo[0] if isinstance(tempo, np.ndarray) and tempo.size > 0 else tempo


        silence_threshold = np.mean(energy) * 0.15
        speech_frames = np.sum(energy > silence_threshold)
        features['speech_ratio'] = speech_frames / len(energy) if len(energy) > 0 else 0.0

        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
        features['spectral_brightness'] = np.mean(spectral_centroids)
        features['brightness_stability'] = 1 / (np.std(spectral_centroids) + 0.1)


    except Exception as e:
        return {
            'avg_energy': 0.01, 'energy_std': 0.01, 'energy_stability': 1,
            'avg_pitch': 100, 'pitch_std': 10, 'pitch_stability': 1,
            'tempo': 120, 'speech_ratio': 0.5, 'spectral_brightness': 2000,
            'brightness_stability': 1,
            'error': str(e)
        }
    return features



def features_to_tags(features: dict):
    """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ç–µ–≥–∏ —Å–æ—Ñ—Ç-—Å–∫–∏–ª–ª–æ–≤."""
    tags = []
    scores = {}
    logging.info("–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ç–µ–≥–∏")
    confidence = 0
    if features.get('energy_stability', 0) > 10: confidence += 0.3
    if features.get('pitch_stability', 0) > 5: confidence += 0.3
    if 0.6 <= features.get('speech_ratio', 0) <= 0.85: confidence += 0.2
    if 80 <= features.get('tempo', 120) <= 140: confidence += 0.2
    if confidence >= 0.6: tags.append("–£–≤–µ—Ä–µ–Ω–Ω—ã–π")
    elif confidence >= 0.3: tags.append("–î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–≤–µ—Ä–µ–Ω–Ω—ã–π")
    else: tags.append("–ù–µ—É–≤–µ—Ä–µ–Ω–Ω—ã–π")
    scores['confidence'] = round(confidence * 100, 1)
    stress_resistance = 0
    if features.get('pitch_std', 100) < 50: stress_resistance += 0.4
    if features.get('energy_std', 1) < 0.02: stress_resistance += 0.3
    if 90 <= features.get('tempo', 120) <= 130: stress_resistance += 0.3
    if stress_resistance >= 0.6: tags.append("–°—Ç—Ä–µ—Å—Å–æ—É—Å—Ç–æ–π—á–∏–≤—ã–π")
    elif stress_resistance >= 0.3: tags.append("–°—Ä–µ–¥–Ω—è—è —Å—Ç—Ä–µ—Å—Å–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å")
    else: tags.append("–ü–æ–¥–≤–µ—Ä–∂–µ–Ω —Å—Ç—Ä–µ—Å—Å—É")
    scores['stress_resistance'] = round(stress_resistance * 100, 1)
    communication = 0
    if features.get('speech_ratio', 0) > 0.7: communication += 0.3
    if features.get('spectral_brightness', 0) > 2000: communication += 0.25
    if features.get('brightness_stability', 0) > 3: communication += 0.25
    if features.get('avg_energy', 0) > 0.03: communication += 0.2
    if communication >= 0.6: tags.append("–û—Ç–ª–∏—á–Ω–∞—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è")
    elif communication >= 0.3: tags.append("–•–æ—Ä–æ—à–∞—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è")
    else: tags.append("–°–ª–∞–±–∞—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è")
    scores['communication'] = round(communication * 100, 1)


    energy_score = 0
    if features.get('avg_energy', 0) > 0.05: energy_score += 0.5
    if 110 <= features.get('tempo', 120) <= 160: energy_score += 0.5
    if energy_score >= 0.6: tags.append("–≠–Ω–µ—Ä–≥–∏—á–Ω—ã–π")
    elif energy_score >= 0.3: tags.append("–£–º–µ—Ä–µ–Ω–Ω–æ –∞–∫—Ç–∏–≤–Ω—ã–π")
    else: tags.append("–ü–∞—Å—Å–∏–≤–Ω—ã–π")
    scores['energy'] = round(energy_score * 100, 1)

    if features.get('tempo', 120) > 150: tags.append("–ë—ã—Å—Ç—Ä–∞—è —Ä–µ—á—å")
    elif features.get('tempo', 120) < 80: tags.append("–ú–µ–¥–ª–µ–Ω–Ω–∞—è —Ä–µ—á—å")
    if features.get('avg_energy', 0) < 0.02: tags.append("–¢–∏—Ö–∏–π –≥–æ–ª–æ—Å")
    elif features.get('avg_energy', 0) > 0.1: tags.append("–ì—Ä–æ–º–∫–∏–π –≥–æ–ª–æ—Å")
    if features.get('pitch_std', 100) < 20: tags.append("–ú–æ–Ω–æ—Ç–æ–Ω–Ω—ã–π")
 
    overall_score = (scores['confidence'] * 0.3 + scores['communication'] * 0.3 + scores['stress_resistance'] * 0.25 + scores['energy'] * 0.15)
    
    logging.info("–¢–µ–≥–∏ –∏ –æ—Ü–µ–Ω–∫–∏ —É—Å–ø–µ—à–Ω–æ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω—ã.")
    
    return {
        'tags': list(set(tags)), # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        'scores': scores,
        'overall_score': round(overall_score, 1)
    }


def analyze_audio_sync(audio_np: np.ndarray, sr: int):
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∞—É–¥–∏–æ."""
    if audio_np.size == 0:
        logging.info("–ü–æ–ø—ã—Ç–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø—É—Å—Ç–æ–≥–æ –∞—É–¥–∏–æ –º–∞—Å—Å–∏–≤–∞.")
        return {
            'tags': ['–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö'],
            'scores': {},
            'error': '–ù–µ—Ç –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'
        }
    
    start_time = time.time()
    features = extract_features(audio_np, sr)
    analysis_result = features_to_tags(features)
    processing_time = (time.time() - start_time) * 1000
    
    analysis_result['meta'] = {
        'total_duration': round(len(audio_np) / sr, 2),
        'processing_time_ms': round(processing_time, 1)
    }
    return analysis_result

def transcribe_audio_api(audio_np: np.ndarray, sr: int):
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–∏–π API."""
    if audio_np.size == 0:
        logging.error("–ü–æ–ø—ã—Ç–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –ø—É—Å—Ç–æ–≥–æ –∞—É–¥–∏–æ –º–∞—Å—Å–∏–≤–∞.")
        return "–ü—É—Å—Ç–æ–π –∞—É–¥–∏–æ —Ñ–∞–π–ª"

    API_KEY = os.getenv("FIREWORKS_API_KEY")
    if not API_KEY:
        logging.error(" –ö–ª—é—á API Fireworks.ai –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return "–û—à–∏–±–∫–∞: –ö–ª—é—á API –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω."

    temp_audio_file = None
    try:
        logging.error(f"–ó–∞–ø—É—Å–∫–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —á–µ—Ä–µ–∑ API...")
        start_time = time.time()

        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
            temp_audio_file = tmpfile.name
            sf.write(tmpfile, audio_np, sr)
            logging.info(f"–ê—É–¥–∏–æ –≤—Ä–µ–º–µ–Ω–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {temp_audio_file}")

        with open(temp_audio_file, "rb") as f:
            response = requests.post(
                "https://audio-turbo.us-virginia-1.direct.fireworks.ai/v1/audio/transcriptions",
                headers={"Authorization": f"Bearer {API_KEY}"},
                files={"file": f},
                data={
                    "model": "whisper-v3-turbo",
                    "temperature": "0",
                    "vad_model": "silero"
                },
            )
        
        processing_time = time.time() - start_time
        logging.info(f"API –æ–±—Ä–∞–±–æ—Ç–∞–ª –∑–∞–ø—Ä–æ—Å –∑–∞ {processing_time:.2f}—Å")

        if response.status_code == 200:
            transcribed_text = response.json().get("text", "").strip()
            if not transcribed_text:
                logging.error("API –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")
                return "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å"
            return transcribed_text
        else:
            logging.error(f"–û—à–∏–±–∫–∞ API: {response.status_code}", response.text)
            return f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤–æ–∑–Ω–∏–∫–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ–º —Ä–µ—á–∏ (–æ—à–∏–±–∫–∞: {response.status_code})."

    except Exception as e:
        logging.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ API: {e}")
        traceback.print_exc()
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤–æ–∑–Ω–∏–∫–ª–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ–º —Ä–µ—á–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–ø–∏—Å–∞—Ç—å –æ—Ç–≤–µ—Ç –µ—â–µ —Ä–∞–∑."
    finally:
        if temp_audio_file and os.path.exists(temp_audio_file):
            os.unlink(temp_audio_file)


@app.websocket("/ws/voice")
async def websocket_voice(websocket: WebSocket):
    print("üîó –ù–æ–≤–æ–µ WebSocket –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ")
    await websocket.accept()
    print("‚úÖ WebSocket —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–Ω—è—Ç–æ")
    
    audio_buffer = io.BytesIO()
    chunks_received = 0
    
    try:
        while True:
            data = await websocket.receive_text()
            message = json.loads(data)
            
            if message['type'] == 'audio_chunk':
                chunk_b64 = message['data']
                chunk_bytes = base64.b64decode(chunk_b64)
                audio_buffer.write(chunk_bytes)
                chunks_received += 1
                
                if chunks_received % 20 == 0:
                    logging.info(f"–ü–æ–ª—É—á–µ–Ω–æ {chunks_received} —á–∞–Ω–∫–æ–≤, –æ–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {audio_buffer.tell()} –±–∞–π—Ç")


            elif message['type'] == 'end':
                logging.info("–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–ø–∏—Å–∏.")
                logging.info(f"–í—Å–µ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {chunks_received}")
                
                audio_bytes = audio_buffer.getvalue()
                
                if len(audio_bytes) == 0:
                    logging.info("–ü–æ–ª—É—á–µ–Ω–æ –ø—É—Å—Ç–æ–µ –∞—É–¥–∏–æ. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É –∫–ª–∏–µ–Ω—Ç—É.")
                    if websocket.application_state == WebSocketState.CONNECTED:
                        await websocket.send_json({'type': 'error', 'message': '–ê—É–¥–∏–æ–∑–∞–ø–∏—Å—å –ø—É—Å—Ç–∞.'})
                    continue


                logging.info(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∞—É–¥–∏–æ —Ä–∞–∑–º–µ—Ä–æ–º {len(audio_bytes)} –±–∞–π—Ç")
                loop = asyncio.get_event_loop()
                
                temp_audio_file = tempfile.NamedTemporaryFile(suffix=".webm", delete=False)
                temp_file_path = temp_audio_file.name
                
                try:
                    temp_audio_file.write(audio_bytes)
                    temp_audio_file.close()
                    logging.info(f"–ê—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∞–Ω—ã –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {temp_file_path}")

                    logging.info("–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞ –≤ numpy array...")
                    start_decode = time.time()
                    audio_np, sr = librosa.load(temp_file_path, sr=16000, mono=True)
                    logging.info(f"–ê—É–¥–∏–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ –∑–∞ {time.time() - start_decode:.2f}—Å. –°—ç–º–ø–ª–æ–≤: {len(audio_np)}, –ß–∞—Å—Ç–æ—Ç–∞: {sr}Hz")

                    transcribe_task = loop.run_in_executor(None, transcribe_audio_api, audio_np, sr)
                    analyze_task = loop.run_in_executor(None, analyze_audio_sync, audio_np, sr)
                    
                    transcription_result = await transcribe_task
                    analysis_result = await analyze_task
                    
                    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: '{transcription_result}'")
                    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞: {analysis_result['tags']}")
                    
                    if websocket.application_state == WebSocketState.CONNECTED:
                        response = {
                            'type': 'final_result',
                            'transcription': transcription_result,
                            'analysis': analysis_result
                        }
                        logging.info(f"–û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∏–µ–Ω—Ç—É...")
                        await websocket.send_json(response)
                        logging.info("–†–µ–∑—É–ª—å—Ç–∞—Ç —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω")
                    else:
                        logging.info("WebSocket –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω, –Ω–µ –º–æ–∂–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç")

                finally:
                    if os.path.exists(temp_file_path):
                        os.unlink(temp_file_path)
                        logging.info(f"üóëÔ∏è –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —É–¥–∞–ª–µ–Ω: {temp_file_path}")

                audio_buffer = io.BytesIO()
                chunks_received = 0
                logging.info("\n–ë—É—Ñ–µ—Ä —Å–±—Ä–æ—à–µ–Ω, –≥–æ—Ç–æ–≤ –∫ –Ω–æ–≤–æ–π –∑–∞–ø–∏—Å–∏.")


    except WebSocketDisconnect:
        logging.info("–ö–ª–∏–µ–Ω—Ç –æ—Ç–∫–ª—é—á–∏–ª—Å—è –æ—Ç WebSocket.")
    except Exception as e:
        logging.info(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ WebSocket: {e}")
        traceback.print_exc()
    finally:
        if not audio_buffer.closed:
            audio_buffer.close()

if __name__ == "__main__":
    import uvicorn

    logging.info("–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ Uvicorn –Ω–∞ http://0.0.0.0:8001")
    uvicorn.run(app, host="0.0.0.0", port=8001, reload=False)
